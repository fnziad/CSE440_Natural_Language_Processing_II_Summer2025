{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bb91e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results for 'Queen - Woman + Man':\n",
      "king: 0.7941\n",
      "royal: 0.6839\n",
      "prince: 0.6763\n",
      "crown: 0.6331\n",
      "vi: 0.6275\n",
      "majesty: 0.6232\n",
      "princess: 0.6215\n",
      "lord: 0.6148\n",
      "palace: 0.6117\n",
      "great: 0.6030\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ===== Load GloVe =====\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_path = \"glove.6B.100d.txt\"  # <-- change path\n",
    "embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "# ===== Cosine Similarity =====\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# ===== Find Most Similar Words =====\n",
    "def most_similar(query_vector, embeddings, top_n=10, exclude=[]):\n",
    "    similarities = {}\n",
    "    for word, vector in embeddings.items():\n",
    "        if word in exclude:\n",
    "            continue\n",
    "        sim = cosine_similarity(query_vector, vector)\n",
    "        similarities[word] = sim\n",
    "    # Sort by similarity score\n",
    "    sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_words[:top_n]\n",
    "\n",
    "# ===== Analogy: Queen - Woman + Man =====\n",
    "def analogy(word_a, word_b, word_c, embeddings, top_n=10):\n",
    "    if word_a not in embeddings or word_b not in embeddings or word_c not in embeddings:\n",
    "        raise ValueError(\"One of the words is not in the vocabulary.\")\n",
    "\n",
    "    vec_a = embeddings[word_a]\n",
    "    vec_b = embeddings[word_b]\n",
    "    vec_c = embeddings[word_c]\n",
    "\n",
    "    # vector_a - vector_b + vector_c\n",
    "    target_vec = vec_a - vec_b + vec_c\n",
    "    results = most_similar(target_vec, embeddings, top_n=top_n, exclude=[word_a, word_b, word_c])\n",
    "    return results\n",
    "\n",
    "# ===== Run the Analogy =====\n",
    "results = analogy(\"queen\", \"woman\", \"man\", embeddings, top_n=10)\n",
    "print(\"Top results for 'Queen - Woman + Man':\")\n",
    "for word, score in results:\n",
    "    print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3b04c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research (Universal ML/DL/NLP/Bio)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
